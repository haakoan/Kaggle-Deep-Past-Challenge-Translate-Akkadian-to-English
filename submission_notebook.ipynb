{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":121150,"databundleVersionId":15061024,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":719070,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":546681,"modelId":557889},{"sourceId":719674,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":547341,"modelId":557889}],"dockerImageVersionId":31236,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Deep Past Challenge - Submission Notebook (ByT5)\n\nThis notebook loads the trained ByT5 model and generates predictions.\n\n**Important:** Uses the same preprocessing as training (ASCII → Diacritics)","metadata":{}},{"cell_type":"code","source":"# ===========================================\n# SETUP\n# ===========================================\n\nimport warnings\nwarnings.simplefilter(\"ignore\")\n\nimport torch\nimport pandas as pd\nimport re\nfrom tqdm.notebook import tqdm\nfrom transformers import AutoTokenizer, T5ForConditionalGeneration\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {DEVICE}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:28:32.714768Z","iopub.execute_input":"2026-01-14T11:28:32.715133Z","iopub.status.idle":"2026-01-14T11:29:13.359645Z","shell.execute_reply.started":"2026-01-14T11:28:32.715108Z","shell.execute_reply":"2026-01-14T11:29:13.358859Z"}},"outputs":[{"name":"stderr","text":"2026-01-14 11:28:53.908446: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1768390134.302985      55 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1768390134.399468      55 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nW0000 00:00:1768390135.257141      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768390135.257189      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768390135.257192      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\nW0000 00:00:1768390135.257195      55 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n","output_type":"stream"},{"name":"stdout","text":"Device: cuda\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ===========================================\n# SETTINGS\n# ===========================================\n\nMODEL_PATH = \"/kaggle/input/keshbyt5/pytorch/small_oldasyrian_finetune/1/byt5_akkadian_small_final\"\nLEXICON_PATH = \"/kaggle/input/deep-past-initiative-machine-translation/OA_Lexicon_eBL.csv\"\n\n# Must match training!\nUSE_LEXICON = True\n\nMAX_SOURCE_LEN = 1024\nMAX_TARGET_LEN = 1024\nBEAM_WIDTH = 4\nREP_PENALTY = 1.5\nNO_REPEAT_NGRAM = 0\nBATCH_SIZE = 4\nPREFIX = \"translate Akkadian to English: \"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:29:13.361441Z","iopub.execute_input":"2026-01-14T11:29:13.362058Z","iopub.status.idle":"2026-01-14T11:29:13.366516Z","shell.execute_reply.started":"2026-01-14T11:29:13.362029Z","shell.execute_reply":"2026-01-14T11:29:13.365670Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# ===========================================\n# LEXICON\n# ===========================================\n\nPN_NAMES = set()\nGN_NAMES = set()\n\nif USE_LEXICON:\n    lex = pd.read_csv(LEXICON_PATH)\n    PN_NAMES = set(lex[lex['type'] == 'PN']['form'].str.lower().dropna())\n    GN_NAMES = set(lex[lex['type'] == 'GN']['form'].str.lower().dropna())\n    print(f\"Lexicon: {len(PN_NAMES)} PNs, {len(GN_NAMES)} GNs\")\nelse:\n    print(\"Lexicon disabled\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:29:13.367392Z","iopub.execute_input":"2026-01-14T11:29:13.367696Z","iopub.status.idle":"2026-01-14T11:29:13.556694Z","shell.execute_reply.started":"2026-01-14T11:29:13.367659Z","shell.execute_reply":"2026-01-14T11:29:13.555743Z"}},"outputs":[{"name":"stdout","text":"Lexicon: 13046 PNs, 328 GNs\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# ===========================================\n# PREPROCESSING (must match training!)\n# ===========================================\n\nASCII_TO_DIACRITIC = {\n    \"sz\": \"š\", \"SZ\": \"Š\", \"Sz\": \"Š\", \"sh\": \"š\", \"SH\": \"Š\", \"Sh\": \"Š\",\n    \"s,\": \"ṣ\", \"S,\": \"Ṣ\", \"t,\": \"ṭ\", \"T,\": \"Ṭ\", \"z,\": \"ẓ\", \"Z,\": \"Ẓ\",\n    \".s\": \"ṣ\", \".S\": \"Ṣ\", \".t\": \"ṭ\", \".T\": \"Ṭ\", \".z\": \"ẓ\", \".Z\": \"Ẓ\",\n    \"h,\": \"ḫ\", \"H,\": \"Ḫ\", \".h\": \"ḫ\", \".H\": \"Ḫ\", \"hh\": \"ḫ\", \"HH\": \"Ḫ\",\n    \"s2\": \"š\", \"S2\": \"Š\", \"s3\": \"ś\", \"S3\": \"Ś\",\n    \"a2\": \"á\", \"a3\": \"à\", \"e2\": \"é\", \"e3\": \"è\",\n    \"i2\": \"í\", \"i3\": \"ì\", \"u2\": \"ú\", \"u3\": \"ù\",\n}\n\nSUBSCRIPTS = {'₀':'0', '₁':'1', '₂':'2', '₃':'3', '₄':'4',\n              '₅':'5', '₆':'6', '₇':'7', '₈':'8', '₉':'9', 'ₓ':'x'}\n\ndef normalize_ascii(text):\n    if not text:\n        return text\n    for k, v in sorted(ASCII_TO_DIACRITIC.items(), key=lambda x: -len(x[0])):\n        text = text.replace(k, v)\n    for k, v in SUBSCRIPTS.items():\n        text = text.replace(k, v)\n    return text\n\ndef normalize_gaps(text):\n    if not text:\n        return text\n    tokens = text.split()\n    result = []\n    i = 0\n    while i < len(tokens):\n        if tokens[i].lower() == \"x\":\n            count = 1\n            while i + count < len(tokens) and tokens[i + count].lower() == \"x\":\n                count += 1\n            result.append(\"<gap>\" if count == 1 else \"<big_gap>\")\n            i += count\n        else:\n            t = tokens[i]\n            if t.lower().startswith(\"x-\"):\n                t = \"<gap>\" + t[1:]\n            elif t.lower().endswith(\"-x\"):\n                t = t[:-1] + \"-<gap>\"\n            result.append(t)\n            i += 1\n    text = \" \".join(result)\n    text = re.sub(r\"(<gap>\\s*){2,}\", \"<big_gap> \", text)\n    text = re.sub(r\"\\.\\.\\.+\", \" <big_gap> \", text)\n    return text.strip()\n\ndef tag_names(text):\n    if not USE_LEXICON or not text:\n        return text\n    words = text.split()\n    result = []\n    for w in words:\n        key = w.replace(\"-\", \"\").lower()\n        if key in PN_NAMES:\n            result.append(f\"[PN]{w}[/PN]\")\n        elif key in GN_NAMES:\n            result.append(f\"[GN]{w}[/GN]\")\n        else:\n            result.append(w)\n    return \" \".join(result)\n\ndef clean_akkadian(text):\n    if pd.isna(text) or not str(text).strip():\n        return \"\"\n    text = str(text)\n    text = text.replace(\"!\", \"\").replace(\"?\", \"\")\n    text = re.sub(r\"[˹˺]\", \"\", text)\n    text = re.sub(r\"\\[([^\\]]+)\\]\", r\"\\1\", text)\n    text = normalize_ascii(text)\n    text = normalize_gaps(text)\n    text = tag_names(text)\n    text = re.sub(r\"\\s+\", \" \", text).strip()\n    return text\n\n# Use this in submission\npreprocess = clean_akkadian","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:29:13.557942Z","iopub.execute_input":"2026-01-14T11:29:13.558431Z","iopub.status.idle":"2026-01-14T11:29:13.573921Z","shell.execute_reply.started":"2026-01-14T11:29:13.558389Z","shell.execute_reply":"2026-01-14T11:29:13.573132Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# ===========================================\n# LOAD MODEL\n# ===========================================\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\nmodel = T5ForConditionalGeneration.from_pretrained(MODEL_PATH).to(DEVICE)\nmodel.eval()\nprint(f\"Model loaded: {model.num_parameters()/1e6:.1f}M params\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:29:13.575161Z","iopub.execute_input":"2026-01-14T11:29:13.575513Z","iopub.status.idle":"2026-01-14T11:29:24.085075Z","shell.execute_reply.started":"2026-01-14T11:29:13.575478Z","shell.execute_reply":"2026-01-14T11:29:24.084330Z"}},"outputs":[{"name":"stdout","text":"Model loaded: 299.6M params\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# ===========================================\n# LOAD TEST DATA\n# ===========================================\n\ntest_df = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/test.csv\")\nprint(f\"Test samples: {len(test_df)}\")\n\n# Preprocess\ntest_df['clean'] = test_df['transliteration'].apply(preprocess)\n\n# Show samples\nprint(\"\\nPreprocessed samples:\")\nfor i in range(3):\n    print(f\"  {test_df.iloc[i]['clean'][:60]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:29:24.086131Z","iopub.execute_input":"2026-01-14T11:29:24.086478Z","iopub.status.idle":"2026-01-14T11:29:24.104957Z","shell.execute_reply.started":"2026-01-14T11:29:24.086454Z","shell.execute_reply":"2026-01-14T11:29:24.104059Z"}},"outputs":[{"name":"stdout","text":"Test samples: 4\n\nPreprocessed samples:\n  um-ma kà-ru-um kà-ni-ia-ma a-na aa-qí-il… da-tim aí-ip-ri-ni...\n  i-na mup-pì-im aa a-lim(ki) ia-tù u„-mì-im a-nim ma-ma-an KÙ...\n  ki-ma mup-pì-ni ta-áa-me-a-ni a-ma-kam lu a-na aí-mì-im a-na...\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"predictions = []\n\nfor i in tqdm(range(0, len(test_df), BATCH_SIZE)):\n    batch_texts = [PREFIX + t for t in test_df['clean'].iloc[i:i+BATCH_SIZE]]\n    batch_lengths = [len(t.encode('utf-8')) for t in test_df['clean'].iloc[i:i+BATCH_SIZE]]\n    \n    inputs = tokenizer(\n        batch_texts, \n        return_tensors='pt', \n        padding=True,\n        truncation=True, \n        max_length=MAX_SOURCE_LEN\n    ).to(DEVICE)\n    \n    # Min length = 50% of input length\n    min_len = int(min(batch_lengths) * 0.5)\n    \n    with torch.no_grad():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=1024,\n            min_new_tokens=min_len,      # Force minimum output\n            num_beams=BEAM_WIDTH,\n            repetition_penalty=REP_PENALTY,\n            no_repeat_ngram_size=NO_REPEAT_NGRAM,\n            length_penalty=1.0,          # Strongly prefer longer\n        )\n    \n    preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n    predictions.extend(preds)\n\ntest_df['translation'] = predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:29:24.107261Z","iopub.execute_input":"2026-01-14T11:29:24.107660Z","iopub.status.idle":"2026-01-14T11:29:28.784536Z","shell.execute_reply.started":"2026-01-14T11:29:24.107633Z","shell.execute_reply":"2026-01-14T11:29:28.783660Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"144273aa280a4f81917ded4418c84ebd"}},"metadata":{}}],"execution_count":7},{"cell_type":"code","source":"# ===========================================\n# SHOW SAMPLES\n# ===========================================\n\nprint(\"\\nSample predictions:\")\nfor i in range(4):\n    print(f\"\\n{i+1}. Input: {test_df.iloc[i]['clean'][:]}...\")\n    print(f\"   Output: {test_df.iloc[i]['translation'][:]}...\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:29:28.785517Z","iopub.execute_input":"2026-01-14T11:29:28.785761Z","iopub.status.idle":"2026-01-14T11:29:28.791885Z","shell.execute_reply.started":"2026-01-14T11:29:28.785741Z","shell.execute_reply":"2026-01-14T11:29:28.791015Z"}},"outputs":[{"name":"stdout","text":"\nSample predictions:\n\n1. Input: um-ma kà-ru-um kà-ni-ia-ma a-na aa-qí-il… da-tim aí-ip-ri-ni kà-ar kà-ar-ma ú wa-bar-ra-tim qí-bi„-ma mup-pu-um aa a-lim(ki) i-li-kam...\n   Output: From the Kanesh colony to Aqil-dātum, our messenger, and say: The letter came to the City.\"...\n\n2. Input: i-na mup-pì-im aa a-lim(ki) ia-tù u„-mì-im a-nim ma-ma-an KÙ.AN i-aa-ú-mu-ni i-na né-mì-lim da-aùr ú-lá e-WA ia-ra-tí-au kà-ru-um kà-ni-ia i-lá-qé...\n   Output: In the letter of the City I myself will not delay the gold in the City. This very day none of the silver is not cleared. The Kanesh colony will take the Kanesh colony....\n\n3. Input: ki-ma mup-pì-ni ta-áa-me-a-ni a-ma-kam lu a-na aí-mì-im a-na É.GAL-lim i-dí-in lu té-ra-at É.GAL-lim ú-kà-lim lu na-aí-ma a-dí-ni lá i-dí-in ma-lá KÙ.AN na-áa-ú ni-bi„-it a-aí-im au-um-au ú au-mì a-bi„-au i-na mup-pì-im lu-up-ta-nim-ma ia-tí aí-ip-ri-ni aé-bi„-lá-nim...\n   Output: As soon as you heard our letter, he gave them to the palace, be it for the other palace, or the terrace of the palace, or the terrace of the palace, or the terrace of the palace, or the name of my father. I shall write to my messenger in the letter, and I shall send it to my messenger....\n\n4. Input: me-+e-er mup-pì-ni a-na kà-ar kà-ar-ma ú wa-bar-ra-tim aé-bi„-lá KÙ.AN lu a-na DAM.GÀR-ru-tim i-dí-in au-mì a-wi-lim lu-up-ta-nim...\n   Output: I have sent our letter to the kar and wabarrātum. He gave the tin to the merchants. Let me write me whether the man should write me....\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ===========================================\n# SUBMISSION\n# ===========================================\n\nsubmission = test_df[['id', 'translation']]\nsubmission.to_csv('submission.csv', index=False)\nprint(f\"Saved {len(submission)} predictions\")\nsubmission.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:29:28.793498Z","iopub.execute_input":"2026-01-14T11:29:28.793722Z","iopub.status.idle":"2026-01-14T11:29:28.866282Z","shell.execute_reply.started":"2026-01-14T11:29:28.793702Z","shell.execute_reply":"2026-01-14T11:29:28.865432Z"}},"outputs":[{"name":"stdout","text":"Saved 4 predictions\n","output_type":"stream"},{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"   id                                        translation\n0   0  From the Kanesh colony to Aqil-dātum, our mess...\n1   1  In the letter of the City I myself will not de...\n2   2  As soon as you heard our letter, he gave them ...\n3   3  I have sent our letter to the kar and wabarrāt...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>translation</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>From the Kanesh colony to Aqil-dātum, our mess...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>In the letter of the City I myself will not de...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>As soon as you heard our letter, he gave them ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>I have sent our letter to the kar and wabarrāt...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install sacrebleu","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:29:28.867426Z","iopub.execute_input":"2026-01-14T11:29:28.867875Z","iopub.status.idle":"2026-01-14T11:29:34.688649Z","shell.execute_reply.started":"2026-01-14T11:29:28.867835Z","shell.execute_reply":"2026-01-14T11:29:34.687638Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.6.0-py3-none-any.whl.metadata (39 kB)\nCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2025.11.3)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (2.0.2)\nRequirement already satisfied: colorama in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from sacrebleu) (5.4.0)\nDownloading sacrebleu-2.6.0-py3-none-any.whl (100 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.8/100.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.2.0-py3-none-any.whl (22 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.2.0 sacrebleu-2.6.0\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ===========================================\n# PARAMETER SEARCH (train subset proxy)\n# now includes no_repeat_ngram_size + encoder_no_repeat_ngram_size\n# ===========================================\n\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport torch\nfrom tqdm.auto import tqdm\nimport sacrebleu\n\nDEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# -------------------------\n# Load pretrained checkpoint\n# -------------------------\n# Set MODEL_DIR to your saved checkpoint folder\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n\nMODEL_DIR = MODEL_PATH\ntokenizer = AutoTokenizer.from_pretrained(MODEL_DIR)\nmodel = AutoModelForSeq2SeqLM.from_pretrained(MODEL_DIR).to(DEVICE)\nmodel.eval()\n\n# -------------------------\n# Data sample (train.csv proxy)\n# -------------------------\ndf = pd.read_csv(\"/kaggle/input/deep-past-initiative-machine-translation/train.csv\")\n\n# preprocess() should exist in your notebook; otherwise define identity:\n# def preprocess(x): return x\n\nN_SAMPLES = 20\nSEED = 42\nBATCH_SIZE = 24\n\nsample = df.sample(N_SAMPLES, random_state=SEED).reset_index(drop=True)\nsample[\"clean\"] = sample[\"transliteration\"].apply(preprocess)\n\n# -------------------------\n# Parameter grid\n# -------------------------\nparam_grid = {\n    \"num_beams\": [2, 4, 6],\n    \"length_penalty\": [0.8, 0.9, 1.0, 1.1, 1.2],\n    \"repetition_penalty\": [1.0, 1.05, 1.10],\n    \"no_repeat_ngram_size\": [0, 3, 4],\n    \"encoder_no_repeat_ngram_size\": [0, 3],\n    \"max_new_tokens\": [128, 256, 384],\n}\n\nkeys = list(param_grid.keys())\ncombinations = list(itertools.product(*[param_grid[k] for k in keys]))\nprint(f\"Testing {len(combinations)} combinations on {len(sample)} samples...\")\n\nresults = []\n\n@torch.no_grad()\ndef decode_batch(texts, gen_kwargs):\n    inputs = tokenizer(\n        [PREFIX + t for t in texts],\n        return_tensors=\"pt\",\n        padding=True,\n        truncation=True,\n        max_length=MAX_SOURCE_LEN,\n    ).to(DEVICE)\n\n    out = model.generate(\n        **inputs,\n        do_sample=False,\n        early_stopping=True if gen_kwargs[\"num_beams\"] > 1 else False,\n        **gen_kwargs,\n    )\n    return tokenizer.batch_decode(out, skip_special_tokens=True)\n\nfor combo in tqdm(combinations):\n    params = dict(zip(keys, combo))\n\n    # Small guard: constraints are only meaningful with beams\n    if params[\"num_beams\"] == 1:\n        params[\"no_repeat_ngram_size\"] = 0\n        params[\"encoder_no_repeat_ngram_size\"] = 0\n\n    preds, refs = [], []\n    for i in range(0, len(sample), BATCH_SIZE):\n        batch = sample.iloc[i:i+BATCH_SIZE]\n        batch_preds = decode_batch(batch[\"clean\"].tolist(), params)\n        preds.extend(batch_preds)\n        refs.extend(batch[\"translation\"].tolist())\n\n    bleu = sacrebleu.corpus_bleu(preds, [refs]).score\n    chrf = sacrebleu.corpus_chrf(preds, [refs], word_order=2).score\n    geo  = float((bleu * chrf) ** 0.5)\n\n    pred_len = np.mean([len(p.split()) for p in preds]) if preds else 0.0\n    ref_len  = np.mean([len(r.split()) for r in refs]) if refs else 0.0\n    len_ratio = (pred_len / ref_len) if ref_len > 0 else np.nan\n\n    results.append({\n        **params,\n        \"BLEU\": bleu,\n        \"chrF++\": chrf,\n        \"GeoMean\": geo,\n        \"len_ratio\": len_ratio,\n    })\n\nresults_df = (\n    pd.DataFrame(results)\n      .sort_values([\"GeoMean\", \"BLEU\"], ascending=False)\n      .reset_index(drop=True)\n)\n\nprint(\"\\nTop 10:\")\ndisplay(results_df.head(10))\n\nprint(\"\\nBest config:\")\nbest = results_df.iloc[0].to_dict()\nbest\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-14T11:29:34.690209Z","iopub.execute_input":"2026-01-14T11:29:34.690530Z"}},"outputs":[{"name":"stdout","text":"Testing 810 combinations on 20 samples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/810 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2589b057a3e4a028657a7e14e298997"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-13T19:50:00.190141Z","iopub.execute_input":"2026-01-13T19:50:00.190955Z","iopub.status.idle":"2026-01-13T19:51:19.949882Z","shell.execute_reply.started":"2026-01-13T19:50:00.190922Z","shell.execute_reply":"2026-01-13T19:51:19.949172Z"}},"outputs":[{"name":"stdout","text":"Testing 5 combinations on 10 samples...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a55ed7b93c3456f884914b869c699f6"}},"metadata":{}},{"name":"stdout","text":"\nTop 5:\n   beams  rep_pen  no_repeat  len_pen        geo\n4      4      1.0          0      2.0  24.610324\n3      4      1.0          0      1.8  24.602071\n2      4      1.0          0      1.5  24.219573\n1      4      1.0          0      1.0  24.110178\n0      4      1.0          0      0.8  23.376068\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}